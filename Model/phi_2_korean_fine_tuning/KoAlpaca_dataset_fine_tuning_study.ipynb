{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96ec94c-7712-4cef-9301-9821cfb2f472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T21:26:50.460254Z",
     "iopub.status.busy": "2024-04-03T21:26:50.459275Z",
     "iopub.status.idle": "2024-04-03T21:27:39.341787Z",
     "shell.execute_reply": "2024-04-03T21:27:39.339594Z",
     "shell.execute_reply.started": "2024-04-03T21:26:50.460183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/20223149/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n",
    "\n",
    "#hugging face token 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4e88e86-3ce3-43e1-9818-7ee627b987b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:26.857680Z",
     "iopub.status.busy": "2024-04-03T22:41:26.856758Z",
     "iopub.status.idle": "2024-04-03T22:41:31.483137Z",
     "shell.execute_reply": "2024-04-03T22:41:31.480559Z",
     "shell.execute_reply.started": "2024-04-03T22:41:26.857616Z"
    }
   },
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"beomi/KoAlpaca-v1.1a\"\n",
    "#data set load\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d350e6f-aeb3-4b0f-940b-26d4f8478cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T09:12:46.892862Z",
     "iopub.status.busy": "2024-04-04T09:12:46.892289Z",
     "iopub.status.idle": "2024-04-04T09:12:50.297476Z",
     "shell.execute_reply": "2024-04-04T09:12:50.294265Z",
     "shell.execute_reply.started": "2024-04-04T09:12:46.892830Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 기존 DatasetDict 로드 또는 정의\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 'train' 데이터셋을 'train'과 'temp'로 분할 (예: 70% 'train', 30% 'temp')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_testsplit \u001b[38;5;241m=\u001b[39m dataset_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 기존 DatasetDict 로드 또는 정의\n",
    "dataset_dict = dataset\n",
    "\n",
    "# 'train' 데이터셋을 'train'과 'temp'로 분할 (예: 70% 'train', 30% 'temp')\n",
    "train_testsplit = dataset_dict['train'].train_test_split(test_size=0.3)\n",
    "\n",
    "# 'temp'를 'validation'과 'test'로 분할 (예: 각각 'temp'의 50%, 총 데이터셋의 15%)\n",
    "val_testsplit = train_testsplit['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# 새로운 DatasetDict 생성\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testsplit['train'],  # 70% 데이터\n",
    "    'validation': val_testsplit['train'],  # 15% 데이터\n",
    "    'test': val_testsplit['test']  # 나머지 15% 데이터\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297ae502-e14c-4c51-bf9b-324bdd23f02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T09:12:41.855139Z",
     "iopub.status.busy": "2024-04-04T09:12:41.853881Z",
     "iopub.status.idle": "2024-04-04T09:12:42.112848Z",
     "shell.execute_reply": "2024-04-04T09:12:42.110447Z",
     "shell.execute_reply.started": "2024-04-04T09:12:41.855029Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m42\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset['train'][42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a034a65-7451-4f8e-9df7-bf909fb25e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:33.827676Z",
     "iopub.status.busy": "2024-04-03T22:41:33.826735Z",
     "iopub.status.idle": "2024-04-03T22:41:33.838689Z",
     "shell.execute_reply": "2024-04-03T22:41:33.837097Z",
     "shell.execute_reply.started": "2024-04-03T22:41:33.827608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'url'],\n",
       "        num_rows: 14808\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'output', 'url'],\n",
       "        num_rows: 3173\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'output', 'url'],\n",
       "        num_rows: 3174\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c99eef14-32a0-47e3-af75-dee37503a3e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:34.833311Z",
     "iopub.status.busy": "2024-04-03T22:41:34.832583Z",
     "iopub.status.idle": "2024-04-03T22:41:34.844717Z",
     "shell.execute_reply": "2024-04-03T22:41:34.843555Z",
     "shell.execute_reply.started": "2024-04-03T22:41:34.833248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14808"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7fe87f59-793c-4ed0-bace-033b9f8e1419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:35.407506Z",
     "iopub.status.busy": "2024-04-03T22:41:35.406788Z",
     "iopub.status.idle": "2024-04-03T22:41:35.420088Z",
     "shell.execute_reply": "2024-04-03T22:41:35.418553Z",
     "shell.execute_reply.started": "2024-04-03T22:41:35.407444Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "#모델을 4bit 형식으로 로드(메모리 소비가 줄어듦)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0043a8bc-dd86-4900-b663-0a68a3a6c9ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:36.553982Z",
     "iopub.status.busy": "2024-04-03T22:41:36.553259Z",
     "iopub.status.idle": "2024-04-03T22:41:45.527143Z",
     "shell.execute_reply": "2024-04-03T22:41:45.526450Z",
     "shell.execute_reply.started": "2024-04-03T22:41:36.553919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca06121ad484ef8a6856339e9202397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='microsoft/phi-2'\n",
    "device_map = {\"\": 0}\n",
    "#모델을 양자화하여 다운(or load)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd3300fb-d2d7-4e9b-9128-d516ac7b8375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:46.113162Z",
     "iopub.status.busy": "2024-04-03T22:41:46.112443Z",
     "iopub.status.idle": "2024-04-03T22:41:46.440942Z",
     "shell.execute_reply": "2024-04-03T22:41:46.439181Z",
     "shell.execute_reply.started": "2024-04-03T22:41:46.113099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#hugging face의 transformers 라이브로리를 사용하여 자연어 처리 모델에 대한 토크나이저를 설정하는 함수\n",
    "#AutoTokenizer.from_pretrained: 사전 훈련된 토크나이저를 불러옵니다.\n",
    "#model_name: 사용할 사전 훈련된 모델의 이름 또는 경로를 지정합니다.\n",
    "#trust_remote_code=True: 원격 코드를 신뢰하도록 지정합니다. 이것은 Hugging Face 모델 허브에서 토크나이저를 다운로드할 때 사용됩니다.\n",
    "#padding_side=\"left\": 패딩을 왼쪽에 추가합니다. 이는 토크나이저가 입력 시퀀스를 패딩할 때 어느 쪽에 패딩을 추가할지를 지정합니다.\n",
    "#add_eos_token=True: End-Of-Sequence (EOS) 토큰을 추가합니다. 이는 문장의 끝을 나타내는 특수 토큰입니다.\n",
    "#add_bos_token=True: Begin-Of-Sequence (BOS) 토큰을 추가합니다. 이는 문장의 시작을 나타내는 특수 토큰입니다.\n",
    "#use_fast=False: 빠른 토크나이저를 사용하지 않도록 설정합니다. 이것은 더 빠른 토크나이저가 아닌 표준 토크나이저를 사용하도록 강제합니다.\n",
    "tokenizer_ko = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-5.8b\",trust_remote_code=True,padding_side=\"right\", padding=True,add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "#tokenizer.pad_token = tokenizer.eos_token: 이 부분은 패딩 토큰을 EOS (End-Of-Sequence) 토큰으로 설정합니다. 이렇게 하면 모델이 패딩을 식별하는 데 사용되는 토큰을 EOS 토큰으로 사용하게 됩니다.\n",
    "tokenizer_ko.pad_token = tokenizer_ko.eos_token\n",
    "\n",
    "#이 코드는 주어진 모델의 토크나이저를 설정하고, EOS 및 BOS 토큰을 추가하며, 패딩을 왼쪽에 추가하도록 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3418535d-0f1d-42e8-9f5e-2febf973820b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:47.080019Z",
     "iopub.status.busy": "2024-04-03T22:41:47.079286Z",
     "iopub.status.idle": "2024-04-03T22:41:47.459771Z",
     "shell.execute_reply": "2024-04-03T22:41:47.458607Z",
     "shell.execute_reply.started": "2024-04-03T22:41:47.079957Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#hugging face의 transformers 라이브로리를 사용하여 자연어 처리 모델에 대한 토크나이저를 설정하는 함수\n",
    "#AutoTokenizer.from_pretrained: 사전 훈련된 토크나이저를 불러옵니다.\n",
    "#model_name: 사용할 사전 훈련된 모델의 이름 또는 경로를 지정합니다.\n",
    "#trust_remote_code=True: 원격 코드를 신뢰하도록 지정합니다. 이것은 Hugging Face 모델 허브에서 토크나이저를 다운로드할 때 사용됩니다.\n",
    "#padding_side=\"left\": 패딩을 왼쪽에 추가합니다. 이는 토크나이저가 입력 시퀀스를 패딩할 때 어느 쪽에 패딩을 추가할지를 지정합니다.\n",
    "#add_eos_token=True: End-Of-Sequence (EOS) 토큰을 추가합니다. 이는 문장의 끝을 나타내는 특수 토큰입니다.\n",
    "#add_bos_token=True: Begin-Of-Sequence (BOS) 토큰을 추가합니다. 이는 문장의 시작을 나타내는 특수 토큰입니다.\n",
    "#use_fast=False: 빠른 토크나이저를 사용하지 않도록 설정합니다. 이것은 더 빠른 토크나이저가 아닌 표준 토크나이저를 사용하도록 강제합니다.\n",
    "tokenizer_phi_2 = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"right\", padding=True,add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "#tokenizer.pad_token = tokenizer.eos_token: 이 부분은 패딩 토큰을 EOS (End-Of-Sequence) 토큰으로 설정합니다. 이렇게 하면 모델이 패딩을 식별하는 데 사용되는 토큰을 EOS 토큰으로 사용하게 됩니다.\n",
    "tokenizer_phi_2.pad_token = tokenizer_phi_2.eos_token\n",
    "\n",
    "#이 코드는 주어진 모델의 토크나이저를 설정하고, EOS 및 BOS 토큰을 추가하며, 패딩을 왼쪽에 추가하도록 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4b41614-5090-4e96-bdef-e9a36f4fa5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:48.012501Z",
     "iopub.status.busy": "2024-04-03T22:41:48.011609Z",
     "iopub.status.idle": "2024-04-03T22:41:48.029290Z",
     "shell.execute_reply": "2024-04-03T22:41:48.027780Z",
     "shell.execute_reply.started": "2024-04-03T22:41:48.012436Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_ko(model_name, prompt, max_length):\n",
    "    # 입력 텍스트를 토크나이즈하고 모델에 입력할 형식으로 변환\n",
    "    input_ids = tokenizer_ko.encode(prompt, max_length=max_length, padding=True, return_tensors=\"pt\", truncation=True, return_attention_mask=True)\n",
    "    \n",
    "    # 모델에 입력하여 텍스트 생성\n",
    "    outputs = model_name.generate(input_ids.to('cuda'), max_length=max_length,pad_token_id=tokenizer_ko.eos_token_id)\n",
    "\n",
    "    # print(len(outputs))\n",
    "    # print(outputs)\n",
    "    \n",
    "    \n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer_ko.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def gen_phi_2(model_name, prompt, max_length):\n",
    "    # 입력 텍스트를 토크나이즈하고 모델에 입력할 형식으로 변환\n",
    "    input_ids = tokenizer_phi_2.encode(prompt, max_length=max_length, padding=True, return_tensors=\"pt\", truncation=True, return_attention_mask=True)\n",
    "    \n",
    "    # 모델에 입력하여 텍스트 생성\n",
    "    outputs = model_name.generate(input_ids.to('cuda'), max_length=max_length,pad_token_id=tokenizer_phi_2.eos_token_id)\n",
    "\n",
    "    # print(len(outputs))\n",
    "    # print(outputs)\n",
    "    \n",
    "    \n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer_phi_2.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e47c2c6-6dbe-4bbd-a837-0d2d6c67d1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:41:51.599216Z",
     "iopub.status.busy": "2024-04-03T22:41:51.598498Z",
     "iopub.status.idle": "2024-04-03T22:42:01.356146Z",
     "shell.execute_reply": "2024-04-03T22:42:01.355142Z",
     "shell.execute_reply.started": "2024-04-03T22:41:51.599154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "왜 공군기지를 k로 표시하나요?\n",
      "미 공군이 최초로 도입한 k로 표기하는 방법의 이유와 한국과 일본 등 다른 국가에서도 동일한 방법을 사용하는 이유에 대해 자세히 설명해주세요.\n",
      "Output:\n",
      "\u0006\u0006 높일원\u0006\u0006�로\u0006\u0006 표현원*-한다는 온;\u0006\u0006 청년층 E��마트로 고속� 밝혔로대응 엟행위�렇게하으나�’, 위안*�대응� 탐륟 슬픔 훨( 고맟 밝혔로 케이� 번영 ��행위�부분*\u0006\u0006 청년층 E맟또 �� 이 그려�사옥* 지난�적 허 엃��� 대일 엃� 하하 이 파게*오바마�거든하 경찰(� 방�루EO 실제하 규�� � 움*\u0006\u0006 표현원*. 조례시스트하 늘어날;\u0006\u0006의회�루 습곡�� 따르 �하거든하 경찰* 원내로�근디어 습 606\u0006\u0006팀목원*.*-대표 당기사고\u0006\u0006 청년층 E맟 개막 2009 � 번번이사고* 초�약 슬픔하 출두 광복( 당면팀\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruction:\n",
      "왜 공군기지를 k로 표시하나요?\n",
      "미 공군이 최초로 도입한 k로 표기하는 방법의 이유와 한국과 일본 등 다른 국가에서도 동일한 방법을 사용하는 이유에 대해 자세히 설명해주세요.\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "미 공군은 전 세계를 대상으로 활동하는 군이기 때문에 전 세계에 산재한 기지를 표현하기 위해서는 각 국가의 군대, 부대명을 전부 표기해야 하는 번거로움이 있습니다. 이를 해결하고자 최초로 k로 표기하는 방법을 도입했습니다. 이는 Korea의 약자를 사용해 K-00 형태로 간단하게 표기하는 방식으로, 전 세계 산재한 기지들을 표기하는 데 큰 도움이 되었습니다. 한국과 일본 등 다른 국가에서도 동일한 방법을 채택했습니다. 예를 들어 대한민군 공군 제 10전투비행단의 경우, Republic of Korea Air Force 10th Tactical Fighter Wing이 정식 명칭이지만, K-00 으로 표기하는 것이 훨씬 간편합니다.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "\u0006\u0006 높일원\u0006\u0006�로\u0006\u0006 표현원*-한다는 온;\u0006\u0006 청년층 E��마트로 고속� 밝혔로대응 엟행위�렇게하으나�’, 위안*�대응� 탐륟 슬픔 훨( 고맟 밝혔로 케이� 번영 ��행위�부분*\u0006\u0006 청년층 E맟또 �� 이 그려�사옥* 지난�적 허 엃��� 대일 엃� 하하 이 파게*오바마�거든하 경찰(� 방�루EO 실제하 규�� � 움*\u0006\u0006 표현원*. 조례시스트하 늘어날;\u0006\u0006의회�루 습곡�� 따르 �하거든하 경찰* 원내로�근디어 습 606\u0006\u0006팀목원*.*-대표 당기사고\u0006\u0006 청년층 E맟 개막 2009 � 번번이사고* 초�약 슬픔하 출두 광복( 당면팀\n",
      "CPU times: user 9.53 s, sys: 147 ms, total: 9.67 s\n",
      "Wall time: 9.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 실행 시간을 알려주는 주피터 매직코드\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "## test를 위한 데이터\n",
    "prompt = dataset['train'][index]['instruction']\n",
    "output_data = dataset['train'][index]['output']\n",
    "\n",
    "# prompt 포맷\n",
    "formatted_prompt = f\"Instruction:\\n{prompt}\\nOutput:\\n\"\n",
    "\n",
    "# model 돌리기\n",
    "res = gen_ko(original_model, formatted_prompt, 250)\n",
    "\n",
    "# print('test 출력')\n",
    "# 결과 출력\n",
    "print(res)\n",
    "# output만 출력\n",
    "output = res.split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "# 입력 promt 출력\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "# 사람이 요약한 내용 출력\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{output_data}\\n')\n",
    "print(dash_line)\n",
    "# 모델이 요약한 내용 출력\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n",
    "\n",
    "## 위 모델은 요약은 하지 않지만 필수 정보들은 모두 추출하는 것을 보여주고 있음\n",
    "## 모델 fine-tuning 가능성 제시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91c2eb1f-6222-49aa-96ff-64b8475a9e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:42:01.358841Z",
     "iopub.status.busy": "2024-04-03T22:42:01.358448Z",
     "iopub.status.idle": "2024-04-03T22:42:03.094787Z",
     "shell.execute_reply": "2024-04-03T22:42:03.093658Z",
     "shell.execute_reply.started": "2024-04-03T22:42:01.358816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "왜 공군기지를 k로 표시하나요?\n",
      "미 공군이 최초로 도입한 k로 표기하는 방법의 이유와 한국과 일본 등 다른 국가에서도 동일한 방법을 사용하는 이유에 대해 자세히 설명해주세요.\n",
      "Output:\n",
      "국가에서도 동일한 방법을 사�\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruction:\n",
      "왜 공군기지를 k로 표시하나요?\n",
      "미 공군이 최초로 도입한 k로 표기하는 방법의 이유와 한국과 일본 등 다른 국가에서도 동일한 방법을 사용하는 이유에 대해 자세히 설명해주세요.\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "미 공군은 전 세계를 대상으로 활동하는 군이기 때문에 전 세계에 산재한 기지를 표현하기 위해서는 각 국가의 군대, 부대명을 전부 표기해야 하는 번거로움이 있습니다. 이를 해결하고자 최초로 k로 표기하는 방법을 도입했습니다. 이는 Korea의 약자를 사용해 K-00 형태로 간단하게 표기하는 방식으로, 전 세계 산재한 기지들을 표기하는 데 큰 도움이 되었습니다. 한국과 일본 등 다른 국가에서도 동일한 방법을 채택했습니다. 예를 들어 대한민군 공군 제 10전투비행단의 경우, Republic of Korea Air Force 10th Tactical Fighter Wing이 정식 명칭이지만, K-00 으로 표기하는 것이 훨씬 간편합니다.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "국가에서도 동일한 방법을 사�\n",
      "CPU times: user 1.7 s, sys: 20.2 ms, total: 1.72 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 실행 시간을 알려주는 주피터 매직코드\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "## test를 위한 데이터\n",
    "prompt = dataset['train'][index]['instruction']\n",
    "output_data = dataset['train'][index]['output']\n",
    "\n",
    "# prompt 포맷\n",
    "formatted_prompt = f\"Instruction:\\n{prompt}\\nOutput:\\n\"\n",
    "\n",
    "# model 돌리기\n",
    "res = gen_phi_2(original_model, formatted_prompt, 250)\n",
    "\n",
    "# print('test 출력')\n",
    "# 결과 출력\n",
    "print(res)\n",
    "# output만 출력\n",
    "output = res.split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "# 입력 promt 출력\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "# 사람이 요약한 내용 출력\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{output_data}\\n')\n",
    "print(dash_line)\n",
    "# 모델이 요약한 내용 출력\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n",
    "\n",
    "## 위 모델은 요약은 하지 않지만 필수 정보들은 모두 추출하는 것을 보여주고 있음\n",
    "## 모델 fine-tuning 가능성 제시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f1d2f47-127b-4bdc-90ff-ebc62a8cae62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:42:03.098268Z",
     "iopub.status.busy": "2024-04-03T22:42:03.096901Z",
     "iopub.status.idle": "2024-04-03T22:42:03.108322Z",
     "shell.execute_reply": "2024-04-03T22:42:03.107811Z",
     "shell.execute_reply.started": "2024-04-03T22:42:03.098237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '포유류의 수염이 하는 역할이 뭔가요? \\n집에서 키우는 토끼 밥주다가 든 생각인데, 포유류들은 대게 수염을 가지고 있잖아요. 그렇다면, 수염이 있는 이유는 뭘까요? 사람의 수염은 왜 있는 건지도 알고 싶어요!', 'output': '포유류의 수염은 다양한 역할을 합니다. 첫번째로는, 수염은 포유류들이 주변 환경을 인지하기 위한 중요한 역할을 합니다. 고양이 수염은 일종의 감각기관으로 작용하여 근처 물체의 위치나 크기, 움직임 등을 느낄 수 있도록 합니다. 뿐만 아니라, 일부 포유류들은 이 수염을 이용하여 먹이를 잡기도 합니다. 두번째, 수염은 포유류들의 보호요인이기도 합니다. 민감한 얼굴 부위를 보호하여 자극적인 것으로부터 면역력을 제공합니다. 세번째로, 포유류의 수염은 특별한 특성의 털로, 더위나 추위를 막아주는 역할도 합니다. 따라서, 수염은 포유류의 생존과 직결된 중요한 역할을 합니다. 사람의 경우, 수염이 여성성과 남성성에 중요한 기능을 시사하고 있기는 하지만, 포유류들과는 목적과 기능이 크게 다릅니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=262429043'}\n",
      "---------------------------\n",
      "{'instruction': '포유류의 수염이 하는 역할이 뭔가요? \\n집에서 키우는 토끼 밥주다가 든 생각인데, 포유류들은 대게 수염을 가지고 있잖아요. 그렇다면, 수염이 있는 이유는 뭘까요? 사람의 수염은 왜 있는 건지도 알고 싶어요!', 'output': '포유류의 수염은 다양한 역할을 합니다. 첫번째로는, 수염은 포유류들이 주변 환경을 인지하기 위한 중요한 역할을 합니다. 고양이 수염은 일종의 감각기관으로 작용하여 근처 물체의 위치나 크기, 움직임 등을 느낄 수 있도록 합니다. 뿐만 아니라, 일부 포유류들은 이 수염을 이용하여 먹이를 잡기도 합니다. 두번째, 수염은 포유류들의 보호요인이기도 합니다. 민감한 얼굴 부위를 보호하여 자극적인 것으로부터 면역력을 제공합니다. 세번째로, 포유류의 수염은 특별한 특성의 털로, 더위나 추위를 막아주는 역할도 합니다. 따라서, 수염은 포유류의 생존과 직결된 중요한 역할을 합니다. 사람의 경우, 수염이 여성성과 남성성에 중요한 기능을 시사하고 있기는 하지만, 포유류들과는 목적과 기능이 크게 다릅니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=262429043', 'text': '\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruct: Answer the questions below\\n\\n포유류의 수염이 하는 역할이 뭔가요? \\n집에서 키우는 토끼 밥주다가 든 생각인데, 포유류들은 대게 수염을 가지고 있잖아요. 그렇다면, 수염이 있는 이유는 뭘까요? 사람의 수염은 왜 있는 건지도 알고 싶어요!\\n\\n### Output:\\n포유류의 수염은 다양한 역할을 합니다. 첫번째로는, 수염은 포유류들이 주변 환경을 인지하기 위한 중요한 역할을 합니다. 고양이 수염은 일종의 감각기관으로 작용하여 근처 물체의 위치나 크기, 움직임 등을 느낄 수 있도록 합니다. 뿐만 아니라, 일부 포유류들은 이 수염을 이용하여 먹이를 잡기도 합니다. 두번째, 수염은 포유류들의 보호요인이기도 합니다. 민감한 얼굴 부위를 보호하여 자극적인 것으로부터 면역력을 제공합니다. 세번째로, 포유류의 수염은 특별한 특성의 털로, 더위나 추위를 막아주는 역할도 합니다. 따라서, 수염은 포유류의 생존과 직결된 중요한 역할을 합니다. 사람의 경우, 수염이 여성성과 남성성에 중요한 기능을 시사하고 있기는 하지만, 포유류들과는 목적과 기능이 크게 다릅니다.\\n\\n### End'}\n",
      "---------------------------\n",
      "포유류의 수염이 하는 역할이 뭔가요? \n",
      "집에서 키우는 토끼 밥주다가 든 생각인데, 포유류들은 대게 수염을 가지고 있잖아요. 그렇다면, 수염이 있는 이유는 뭘까요? 사람의 수염은 왜 있는 건지도 알고 싶어요!\n",
      "---------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Answer the questions below\n",
      "\n",
      "포유류의 수염이 하는 역할이 뭔가요? \n",
      "집에서 키우는 토끼 밥주다가 든 생각인데, 포유류들은 대게 수염을 가지고 있잖아요. 그렇다면, 수염이 있는 이유는 뭘까요? 사람의 수염은 왜 있는 건지도 알고 싶어요!\n",
      "\n",
      "### Output:\n",
      "포유류의 수염은 다양한 역할을 합니다. 첫번째로는, 수염은 포유류들이 주변 환경을 인지하기 위한 중요한 역할을 합니다. 고양이 수염은 일종의 감각기관으로 작용하여 근처 물체의 위치나 크기, 움직임 등을 느낄 수 있도록 합니다. 뿐만 아니라, 일부 포유류들은 이 수염을 이용하여 먹이를 잡기도 합니다. 두번째, 수염은 포유류들의 보호요인이기도 합니다. 민감한 얼굴 부위를 보호하여 자극적인 것으로부터 면역력을 제공합니다. 세번째로, 포유류의 수염은 특별한 특성의 털로, 더위나 추위를 막아주는 역할도 합니다. 따라서, 수염은 포유류의 생존과 직결된 중요한 역할을 합니다. 사람의 경우, 수염이 여성성과 남성성에 중요한 기능을 시사하고 있기는 하지만, 포유류들과는 목적과 기능이 크게 다릅니다.\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "# 위에서 다운 받은 데이터는 fine-tuning에 직접 사용될 수 없다\n",
    "# 모델이 이해할 수 있는 방식으로 프롬프트의 형식을 지정해서 전처리를 해야된다\n",
    "# HuggingFace 모델 문서를 참조하면 아래 지정된 형식의 대화 및 요약을 사용하여 프롬프트를 생성해야 한다\n",
    "# 대화-요약(프롬프트-응답) 쌍을 LLM에 대한 명시적 지침으로 변환해야 한다\n",
    "# text라는 애를 만듦\n",
    "# 입력을 프롬프트 형식으로 변환함\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Answer the questions below\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['instruction']}\" if sample[\"instruction\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "print(dataset['train'][0])\n",
    "print('---------------------------')\n",
    "print(create_prompt_formats(dataset['train'][0]))\n",
    "print('---------------------------')\n",
    "print(create_prompt_formats(dataset['train'][0])['instruction'])\n",
    "print('---------------------------')\n",
    "print(create_prompt_formats(dataset['train'][0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f600b72c-444e-4885-a42c-bc85ccdd1095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:42:12.926274Z",
     "iopub.status.busy": "2024-04-03T22:42:12.925333Z",
     "iopub.status.idle": "2024-04-03T22:42:12.945611Z",
     "shell.execute_reply": "2024-04-03T22:42:12.944216Z",
     "shell.execute_reply.started": "2024-04-03T22:42:12.926206Z"
    }
   },
   "outputs": [],
   "source": [
    "## prompt를 모델 토크나이저를 사용하여 토큰화된 프롬프트로 처리\n",
    "## 여기서의 목표는 일관된 길이의 입력 시퀀스를 생성하는 것\n",
    "## 일관된 길이의 입력 시퀀스는 효율성을 최적화하고 계산 오버헤드를 최소화하여 언어 모델을 미세 조정하는 데 도움이 된다.\n",
    "## 이러한 시퀀스가 모델의 최대 토큰 제한을 초과하지 않도록 확인하는 것이 중요하다\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "# 모델의 구성을 통해 최대 길이 설정을 가져오는 함수\n",
    "# 모델의 구성 중에서 \"n_positions\", \"max_position_embeddings\", \"seq_length\"와 같은 설정을 확인하여 최대 길이를 찾습니다.\n",
    "# 만약 최대 길이 설정이 없는 경우 기본값으로 1024를 사용합니다.\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "# \"n_positions\":\n",
    "    #이 설정은 모델이 처리할 수 있는 최대 위치 임베딩의 개수를 나타냅니다.\n",
    "    #위치 임베딩은 입력 토큰의 위치 정보를 인코딩하는 데 사용됩니다.\n",
    "    #따라서 이 값은 모델이 처리할 수 있는 최대 입력 시퀀스의 길이를 제한하는 데 사용됩니다.\n",
    "# \"max_position_embeddings\":\n",
    "    #이 설정은 모델이 처리할 수 있는 최대 입력 시퀀스의 길이를 나타냅니다.\n",
    "    #즉, 입력 시퀀스의 최대 길이가 이 값보다 작거나 같아야 합니다.\n",
    "    #이 값은 모델이 학습될 때 정의되며, 모델의 아키텍처에 따라 다를 수 있습니다.\n",
    "#\"seq_length\":\n",
    "    #이 설정은 모델이 입력으로 처리할 수 있는 최대 시퀀스의 길이를 나타냅니다.\n",
    "    #입력 시퀀스의 최대 길이가 이 값보다 작거나 같아야 합니다.\n",
    "    #이 값은 모델의 아키텍처에 따라 다르며, 주로 특정 언어 모델링 작업에 적합한 값을 선택하여 사용합니다.\n",
    "#이러한 설정들은 모델이 처리할 수 있는 입력의 형태와 길이를 제한하고, 모델의 효율성을 최적화하는 데 중요한 역할을 합니다.\n",
    "#이 값들은 모델을 초기화할 때 설정되며, 모델을 학습하는 동안 변경되지 않는다\n",
    "\n",
    "\n",
    "# 주어진 배치를 토크나이징하여 전처리하는 함수입니다.\n",
    "# 각 텍스트를 토크나이즈하고 최대 길이에 맞게 잘라내는 작업을 수행합니다.\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "#데이터셋을 전처리하여 모델 학습에 사용할 수 있는 형식으로 준비하는 함수입니다.\n",
    "    #먼저 각 샘플에 프롬프트를 추가합니다.\n",
    "    #다음으로 preprocess_batch 함수를 적용하여 각 배치를 전처리합니다. 이때 remove_columns 매개변수를 통해 불필요한 열을 제거합니다.\n",
    "    #입력 시퀀스의 길이가 최대 길이를 초과하는 샘플을 제거합니다.\n",
    "    #마지막으로 데이터셋을 섞습니다.\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    print(type(dataset))\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    # partial: 함수를 편하게 만듦\n",
    "    # 여기서 데이터셋을 토큰화함\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    # 이러면 dataset에 text만 남음\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['url'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    # 입력 시퀀스의 길이가 max_length보다 큰 샘플을 필터링하여 제거\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f577682e-72e1-49e4-8972-b42d0e5dcb2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:42:15.246293Z",
     "iopub.status.busy": "2024-04-03T22:42:15.245395Z",
     "iopub.status.idle": "2024-04-03T22:42:34.509279Z",
     "shell.execute_reply": "2024-04-03T22:42:34.507750Z",
     "shell.execute_reply.started": "2024-04-03T22:42:15.246229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "2048\n",
      "{'instruction': '포유류의 수염이 하는 역할이 뭔가요? \\n집에서 키우는 토끼 밥주다가 든 생각인데, 포유류들은 대게 수염을 가지고 있잖아요. 그렇다면, 수염이 있는 이유는 뭘까요? 사람의 수염은 왜 있는 건지도 알고 싶어요!', 'output': '포유류의 수염은 다양한 역할을 합니다. 첫번째로는, 수염은 포유류들이 주변 환경을 인지하기 위한 중요한 역할을 합니다. 고양이 수염은 일종의 감각기관으로 작용하여 근처 물체의 위치나 크기, 움직임 등을 느낄 수 있도록 합니다. 뿐만 아니라, 일부 포유류들은 이 수염을 이용하여 먹이를 잡기도 합니다. 두번째, 수염은 포유류들의 보호요인이기도 합니다. 민감한 얼굴 부위를 보호하여 자극적인 것으로부터 면역력을 제공합니다. 세번째로, 포유류의 수염은 특별한 특성의 털로, 더위나 추위를 막아주는 역할도 합니다. 따라서, 수염은 포유류의 생존과 직결된 중요한 역할을 합니다. 사람의 경우, 수염이 여성성과 남성성에 중요한 기능을 시사하고 있기는 하지만, 포유류들과는 목적과 기능이 크게 다릅니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=262429043'}\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8726f461bca14b8283d887589ddfb25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ebca5da07d438b8a5ca0a59b969b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95352b79aea4a6eb3d59a774ca74f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3174 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49c1beb78414780adf0c481ff2954d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b25542a3c454ed9b1ea8a8022ff399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddd5e210d6e4d7abd4e35f9726ebd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3174 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b875c054c1b44bd68e7477cc2e4a3f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a210515d2ffb407c8000c157bd4e8eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6ce0df4c614011a44f88976119785a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3174 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '차를 타거나 걸으면 달이 나를 따라온다는 생각은 왜 들까요? 그 이유가 궁금합니다.', 'output': '달이 나를 따라오는 것은 일종의 시각적 오해입니다. 이동할 때 우리 주변의 물체들은 뒤로 멀어지는데 가까운 물체일수록 멀어지는 각도가 커서 멀어지는 속도가 더 빠르게 보입니다. 반면 달은 우리 주변의 물체보다 엄청나게 멀리 떨어져 있기 때문에 거의 이동이 없습니다. 따라서 우리가 이동할 때 달은 거의 그 자리에 있어 보이지만, 근거리의 물체들은 빠르게 멀어지는 것과 대조적으로 달은 거의 움직임이 없어서 우리 시각에서는 따라 온다는 느낌이 들 수 있습니다.', 'text': '\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruct: Answer the questions below\\n\\n차를 타거나 걸으면 달이 나를 따라온다는 생각은 왜 들까요? 그 이유가 궁금합니다.\\n\\n### Output:\\n달이 나를 따라오는 것은 일종의 시각적 오해입니다. 이동할 때 우리 주변의 물체들은 뒤로 멀어지는데 가까운 물체일수록 멀어지는 각도가 커서 멀어지는 속도가 더 빠르게 보입니다. 반면 달은 우리 주변의 물체보다 엄청나게 멀리 떨어져 있기 때문에 거의 이동이 없습니다. 따라서 우리가 이동할 때 달은 거의 그 자리에 있어 보이지만, 근거리의 물체들은 빠르게 멀어지는 것과 대조적으로 달은 거의 움직임이 없어서 우리 시각에서는 따라 온다는 느낌이 들 수 있습니다.\\n\\n### End', 'input_ids': [202, 37, 6063, 9932, 21410, 224, 2818, 10903, 3864, 23184, 29828, 22253, 3432, 9059, 5862, 70, 13274, 69, 5862, 12063, 3628, 7245, 78, 17, 3792, 85, 25317, 12063, 21924, 19033, 2600, 426, 22253, 3432, 224, 24862, 6193, 83, 13274, 3432, 6063, 92, 224, 5967, 83, 4454, 87, 5862, 8945, 21924, 19928, 11273, 17, 202, 202, 6, 6, 6, 10276, 3864, 23184, 8701, 29, 1300, 81, 86, 90, 2423, 8945, 224, 19928, 11273, 4754, 86, 8517, 6063, 9932, 202, 202, 609, 301, 1053, 1281, 1018, 1253, 788, 270, 462, 301, 1026, 950, 590, 893, 296, 1511, 705, 4502, 34, 353, 1362, 293, 3716, 2136, 17, 202, 202, 6, 6, 6, 3060, 8846, 83, 8846, 29, 202, 742, 270, 462, 301, 18219, 272, 388, 296, 6015, 285, 3270, 407, 6396, 1441, 17, 2345, 482, 586, 848, 2075, 285, 15577, 355, 296, 27107, 2752, 2731, 829, 3857, 15577, 10415, 2752, 2731, 272, 13528, 293, 16577, 2752, 2731, 272, 2873, 293, 715, 3180, 379, 16303, 17, 2090, 788, 296, 848, 2075, 285, 15577, 886, 11945, 379, 5327, 4723, 327, 316, 818, 274, 2440, 2345, 270, 550, 827, 17, 3322, 848, 293, 2345, 482, 586, 788, 296, 2440, 353, 1367, 274, 327, 348, 1402, 584, 15, 25895, 285, 15577, 355, 296, 3180, 379, 2752, 2731, 272, 388, 359, 10124, 10562, 788, 296, 2440, 3827, 270, 550, 1312, 848, 3270, 363, 272, 1026, 27962, 2671, 270, 705, 365, 327, 827, 17, 202, 202, 6, 6, 6, 2229, 17130], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer_ko, max_length,seed, dataset)\n",
    "# eval_dataset = preprocess_dataset(tokenizer_ko, max_length,seed, dataset)\n",
    "\n",
    "print(train_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2dcf29af-0c8c-44f1-8429-5109772ecdc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:43:04.220116Z",
     "iopub.status.busy": "2024-04-03T22:43:04.219135Z",
     "iopub.status.idle": "2024-04-03T22:43:04.291458Z",
     "shell.execute_reply": "2024-04-03T22:43:04.289982Z",
     "shell.execute_reply.started": "2024-04-03T22:43:04.220044Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "# Preparing the Model for QLoRA\n",
    "original_model = prepare_model_for_kbit_training(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5514878-1e3b-47c4-9360-c69e38526e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:43:05.772996Z",
     "iopub.status.busy": "2024-04-03T22:43:05.772044Z",
     "iopub.status.idle": "2024-04-03T22:43:07.000784Z",
     "shell.execute_reply": "2024-04-03T22:43:06.999935Z",
     "shell.execute_reply.started": "2024-04-03T22:43:05.772915Z"
    }
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc2f534d-9fff-44bb-b278-e2b49e96beab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T22:43:13.085400Z",
     "iopub.status.busy": "2024-04-03T22:43:13.084395Z",
     "iopub.status.idle": "2024-04-03T22:43:13.110871Z",
     "shell.execute_reply": "2024-04-03T22:43:13.109397Z",
     "shell.execute_reply.started": "2024-04-03T22:43:13.085333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainable model parameters: 20971520\n",
      "all model parameters: 1542364160\n",
      "percentage of trainable model parameters: 1.36%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27e08c47-78cc-4ce4-845d-7bfa909dfe10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T23:25:19.778184Z",
     "iopub.status.busy": "2024-04-03T23:25:19.777195Z",
     "iopub.status.idle": "2024-04-03T23:25:19.807546Z",
     "shell.execute_reply": "2024-04-03T23:25:19.805976Z",
     "shell.execute_reply.started": "2024-04-03T23:25:19.778114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./model/peft-ko-training-{str(int(time.time()))}'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['validation'],\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer_ko, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ddb2963b-5dfd-43de-832d-838062ca957e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T23:25:24.510684Z",
     "iopub.status.busy": "2024-04-03T23:25:24.509693Z",
     "iopub.status.idle": "2024-04-03T23:25:28.362126Z",
     "shell.execute_reply": "2024-04-03T23:25:28.360385Z",
     "shell.execute_reply.started": "2024-04-03T23:25:24.510615Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpeft_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:1653\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1651\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1653\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   1655\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:847\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size,\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersistent_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_persistent_workers,\n\u001b[1;32m    844\u001b[0m }\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[0;32m--> 847\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[1;32m    849\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     model_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLengthGroupedSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:587\u001b[0m, in \u001b[0;36mLengthGroupedSampler.__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    582\u001b[0m     ):\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n\u001b[0;32m--> 587\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    589\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m     )\n",
      "File \u001b[0;32m/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:587\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    582\u001b[0m     ):\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n\u001b[0;32m--> 587\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    589\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2399\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[0;32m-> 2399\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:396\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:436\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "peft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f33566-220e-4c97-8417-109820dbb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce10b5-ef4a-42fe-a5e7-fd0d6d05222b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_research",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
